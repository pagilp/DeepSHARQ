{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d83f637",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca099f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 10:25:12.789154: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-08 10:25:12.896158: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-08 10:25:12.896172: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-08 10:25:13.350623: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-08 10:25:13.350689: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-08 10:25:13.350695: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/pablo/.local/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.8.0 and strictly below 2.11.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.11.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import onnx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from onnx_tf.backend import prepare\n",
    "import tensorflow as tf\n",
    "\n",
    "from pytorch_helper import DeepHEC,get_test_dataset\n",
    "from tf_helper import predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7798dd7",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "875a43b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs and batch size\n",
    "MAX_EPOCH = 1000\n",
    "\n",
    "# NN architecture\n",
    "HIDDEN = 4\n",
    "NEURONS = 150\n",
    "INPUT = 6\n",
    "OUTPUT = 256\n",
    "\n",
    "# Learning rate schedule configuration\n",
    "MAX_LR = 4e-2\n",
    "START_LR = 0.001\n",
    "END_LR = 2e-9\n",
    "DIV_FACTOR = MAX_LR / START_LR\n",
    "FINAL_DIV_FACTOR = MAX_LR / END_LR\n",
    "\n",
    "# Regularization factor\n",
    "REG_FACTOR = 0.00001\n",
    "\n",
    "# DeepSHARQ delta\n",
    "RI_RANGE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4250f",
   "metadata": {},
   "source": [
    "## Prepare Path Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edaaf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"DeepSHARQ\"\n",
    "\n",
    "if MODEL == \"DeepSHARQ\":\n",
    "    PATH = \"./models/leakyReLU({},{})_K_range_{}_{}_max_lr={}_start_lr={}_final_lr={}_reg_factor={}/\".format(HIDDEN,NEURONS,RI_RANGE,MAX_EPOCH,MAX_LR,START_LR,END_LR,REG_FACTOR)\n",
    "elif MODEL == \"DeepHEC\":\n",
    "    PATH = \"./models/leakyReLU({},{})_fullmodel_{}_max_lr={}_div_factor={}_final_div_factor={}_reg_factor={}/\".format(HIDDEN,NEURONS,MAX_EPOCH,MAX_LR,DIV_FACTOR,FINAL_DIV_FACTOR,REG_FACTOR)\n",
    "\n",
    "PATH = \"./models/\"    \n",
    "\n",
    "PYTORCH_PATH = PATH + \"model\"\n",
    "ONNX_PATH = PATH + \"model.onnx\"\n",
    "TF_PATH = PATH + \"model.pb\"\n",
    "TFLITE_PATH = PATH + \"model.tflite\"\n",
    "DATASET_PATH = './datasets/dataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873b788",
   "metadata": {},
   "source": [
    "## Load Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86acd469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = DeepHEC(hidden_layers=HIDDEN, layer_size=NEURONS, inputs=INPUT, outputs=OUTPUT)\n",
    "trained_model.load_state_dict(torch.load(PYTORCH_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e551146",
   "metadata": {},
   "source": [
    "## Convert PyTorch to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "041047c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import onnx_tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee4e944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 10:25:14.226216: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-06-08 10:25:14.226248: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-08 10:25:14.226265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pablo-pc): /proc/driver/nvidia/version does not exist\n",
      "2023-06-08 10:25:14.226464: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:Function `__call__` contains input name(s) onnx_tf__tf_Gemm_0_0b983715 with unsupported characters which will be renamed to onnx_tf__tf_gemm_0_0b983715 in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/model.pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/model.pb/assets\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1,6)\n",
    "torch.onnx.export(trained_model, dummy_input, ONNX_PATH)\n",
    "\n",
    "onnx_model = onnx.load(ONNX_PATH)\n",
    "tf_rep = prepare(onnx_model)\n",
    "tf_rep.export_graph(TF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4829a",
   "metadata": {},
   "source": [
    "## Convert ONNX to TFlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df69f7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 10:25:15.243952: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-06-08 10:25:15.243988: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-06-08 10:25:15.244998: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./models/model.pb\n",
      "2023-06-08 10:25:15.245927: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-06-08 10:25:15.245942: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./models/model.pb\n",
      "2023-06-08 10:25:15.247873: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-06-08 10:25:15.248151: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-06-08 10:25:15.292403: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./models/model.pb\n",
      "2023-06-08 10:25:15.296503: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 51831 microseconds.\n",
      "2023-06-08 10:25:15.321790: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-08 10:25:15.385256: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2111] Estimated count of arithmetic ops: 0.214 M  ops, equivalently 0.107 M  MACs\n"
     ]
    }
   ],
   "source": [
    "# make a converter object from the saved tensorflow file\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(TF_PATH)\n",
    "\n",
    "# I had to explicitly state the ops\n",
    "converter.target_spec.supported_ops = [tf.compat.v1.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                                       tf.compat.v1.lite.OpsSet.SELECT_TF_OPS]\n",
    "\n",
    "tf_lite_model = converter.convert()\n",
    "# Save the model.\n",
    "with open(TFLITE_PATH, 'wb') as f:\n",
    "    f.write(tf_lite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70207cfd",
   "metadata": {},
   "source": [
    "## Test Converted Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21cc8010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "test_dataset = get_test_dataset(DATASET_PATH)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Load TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "# Get input and outpt tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fba4760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`onnx::Gemm_0` is not a valid node name. Accepted names conform to Regex /re.compile('^[A-Za-z0-9.][A-Za-z0-9_.\\\\\\\\/>-]*$')/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`onnx::Gemm_0` is not a valid node name. Accepted names conform to Regex /re.compile('^[A-Za-z0-9.][A-Za-z0-9_.\\\\\\\\/>-]*$')/\n"
     ]
    }
   ],
   "source": [
    "# Check that the three models output the same result\n",
    "with torch.no_grad():\n",
    "    error_tf = 0\n",
    "    error_tflite = 0\n",
    "    total = 0\n",
    "    for x, y in test_dataloader:\n",
    "        # Predict with the three models\n",
    "        out_py = trained_model(x.clone())\n",
    "        out_tf = tf_rep.run(pd.DataFrame([pd.Series(x[0].tolist(), dtype=\"float32\")]), verbose=0)[0][0]\n",
    "        out_tflite = predict(interpreter, [np.float32(x) for x in x[0].tolist()], input_details, output_details)\n",
    "        \n",
    "        # Obtain model output\n",
    "        out_py_argmax = out_py.argmax().item()\n",
    "        out_tf_argmax = out_tf.argmax()\n",
    "        out_tflite_argmax = out_tflite\n",
    "        \n",
    "        # Check for prediction erros in conversions\n",
    "        \n",
    "        if out_py_argmax != out_tf_argmax:\n",
    "            print(\"Error TF\")\n",
    "            error_tf += 1\n",
    "        elif out_py_argmax != out_tflite_argmax:\n",
    "            print(\"Error TFLite\")\n",
    "            error_tflite += 1\n",
    "        total += 1\n",
    "\n",
    "print(\"Error TF {}, Error TFlite {}\".format(error_tf/total, error_tflite/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
